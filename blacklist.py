import urllib.request
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from datetime import datetime
import os
import re #æ­£åˆ™
from urllib.parse import urlparse


timestart = datetime.now()

# è¯»å–æ–‡ä»¶å†…å®¹
def read_txt_file(file_path):
    skip_strings = ['#genre#', '127.0.0.1', '192.168', '198.168', 'ChiSheng9', 'epg.pw', 'p3p', '/udp/', '(576p)', '(540p)', '(360p)', '(480p)', '(180p)', '(404p)', 'r.jdshipin', 'generationnexxxt', 'live.goodiptv.club', 'playtv-live.ifeng']  # å®šä¹‰éœ€è¦è·³è¿‡çš„å­—ç¬¦ä¸²æ•°ç»„['#', '@', '#genre#'] 
    required_strings = ['://']  # å®šä¹‰éœ€è¦åŒ…å«çš„å­—ç¬¦ä¸²æ•°ç»„['å¿…éœ€å­—ç¬¦1', 'å¿…éœ€å­—ç¬¦2'] 

    with open(file_path, 'r', encoding='utf-8') as file:
        lines = [
            line for line in file
            if not any(skip_str in line for skip_str in skip_strings) and all(req_str in line for req_str in required_strings)
        ]
    return lines

def read_txt(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        return f.readlines()

def append_to_file(filename, lines):
    with open(filename, 'a', encoding='utf-8') as f:
        for line in lines:
            f.write(line)
            
# æ ¼å¼åŒ–é¢‘é“åç§°
def process_name_string(input_str):
    parts = input_str.split(',')
    processed_parts = []
    for part in parts:
        processed_part = process_part(part)
        processed_parts.append(processed_part)
    result_str = ','.join(processed_parts)
    return result_str

def process_part(part_str):
    # å¤„ç†é€»è¾‘
    part_str = part_str.replace("ã€ŒIPV6ã€", "")  # å‰”é™¤ ã€ŒIPV6ã€
    part_str = part_str.replace("IPV6", "")  # å‰”é™¤ IPV6
    part_str = part_str.replace("ã€ŒIPV4ã€", "")  # å‰”é™¤ ã€ŒIPV4ã€
    part_str = part_str.replace("IPV4", "")  # å‰”é™¤ IPV4 
    part_str = part_str.replace("[V4]", "")  # å‰”é™¤ [V4]
    part_str = part_str.replace("[V6]", "")  # å‰”é™¤ [V6]
    part_str = part_str.replace("å°,http", ",http")  # æ›¿æ¢ å°
    part_str = part_str.replace("é«˜æ¸…,http", ",http")  # æ›¿æ¢ é«˜æ¸…
    part_str = part_str.replace("æ ‡æ¸…,http", ",http")  # æ›¿æ¢ æ ‡æ¸…  
    part_str = part_str.replace("è¦–", "è§†")  # æ›¿æ¢
    part_str = part_str.replace("è", "é—»")  # æ›¿æ¢
    part_str = part_str.replace("è¡›", "å«")  # æ›¿æ¢
    part_str = part_str.replace("æ±", "ä¸œ")  # æ›¿æ¢
    part_str = part_str.replace("è¯", "å")  # æ›¿æ¢
    part_str = part_str.replace("é›»", "ç”µ")  # æ›¿æ¢
    part_str = part_str.replace("èª", "è¯­")  # æ›¿æ¢
    part_str = part_str.replace("é–“", "é—´")  # æ›¿æ¢
    part_str = part_str.replace("äº", "äºš")  # æ›¿æ¢
    part_str = part_str.replace("ç·š", "çº¿")  # æ›¿æ¢
    part_str = part_str.replace("åœ‹", "å›½")  # æ›¿æ¢
    part_str = part_str.replace("ç£", "æ¹¾")  # æ›¿æ¢
    part_str = part_str.replace("ç’°", "ç¯")  # æ›¿æ¢
    part_str = part_str.replace("è“®", "è²")  # æ›¿æ¢
    part_str = part_str.replace("é¡", "é•œ")  # æ›¿æ¢
    part_str = part_str.replace("è²¡ç¶“", "è´¢ç»")  # æ›¿æ¢
    part_str = part_str.replace("å‡¤å‡°-", "å‡¤å‡°")  # æ›¿æ¢
    part_str = part_str.replace("é³³å‡°", "å‡¤å‡°")  # æ›¿æ¢
    part_str = part_str.replace("è³‡è¨Š", "èµ„è®¯")  # æ›¿æ¢
    part_str = part_str.replace("ç´€å¯¦", "çºªå®")  # æ›¿æ¢
    part_str = part_str.replace(" HD", "")  # å‰”é™¤ HD
    part_str = part_str.replace("HD", "")  # å‰”é™¤ HD
    part_str = part_str.replace("ğŸ˜", "0")  # æ›¿æ¢ ğŸ˜
    part_str = part_str.replace("ğŸ™", "1")  # æ›¿æ¢ ğŸ™
    part_str = part_str.replace("ğŸš", "2")  # æ›¿æ¢ ğŸš
    part_str = part_str.replace("ğŸ›", "3")  # æ›¿æ¢ ğŸ›
    part_str = part_str.replace("ğŸœ", "4")  # æ›¿æ¢ ğŸœ
    part_str = part_str.replace("ğŸ", "5")  # æ›¿æ¢ ğŸ
    part_str = part_str.replace("ğŸ", "6")  # æ›¿æ¢ ğŸ
    part_str = part_str.replace("ğŸŸ", "7")  # æ›¿æ¢ ğŸŸ
    part_str = part_str.replace("ğŸ ", "8")  # æ›¿æ¢ ğŸ 
    part_str = part_str.replace("ğŸ¡", "9")  # æ›¿æ¢ ğŸ¡
    part_str = part_str.replace("ç§»åŠ¨å’ªå’•ç›´æ’­", "å’ªå’•ä½“è‚²")  # æ›¿æ¢ ç§»åŠ¨å’ªå’•ç›´æ’­
    part_str = part_str.replace("å’ªå’•ç›´æ’­", "å’ªå’•ä½“è‚²")  # æ›¿æ¢ å’ªå’•ç›´æ’­
    part_str = part_str.replace("å’ªå’•è§†é¢‘", "å’ªå’•ä½“è‚²")  # æ›¿æ¢ å’ªå’•è§†é¢‘
    part_str = part_str.replace("å’ªå’•ä½“è‚²-", "å’ªå’•ä½“è‚²")  # æ›¿æ¢ å’ªå’•ä½“è‚²
    part_str = part_str.replace("å’ªå’•ä½“è‚²_", "å’ªå’•ä½“è‚²")  # æ›¿æ¢ å’ªå’•ä½“è‚²
    part_str = part_str.replace("â€¢", "")  # å…ˆå‰”é™¤ â€¢  
    part_str = part_str.replace("_4M1080HEVC", "")  # å‰”é™¤
    part_str = part_str.replace("_2.5M1080HEVC", "")  # å‰”é™¤
    part_str = part_str.replace(" (1080p)", "")  # æ›¿æ¢ 1080p
    part_str = part_str.replace(" (900p)", "")  # æ›¿æ¢ 900p
    part_str = part_str.replace(" (720p)", "")  # æ›¿æ¢ 720p
    part_str = part_str.replace(" (576p)", "")  # æ›¿æ¢ 576p
    part_str = part_str.replace(" (540p)", "")  # æ›¿æ¢ 540p
    part_str = part_str.replace(" (480p)", "")  # æ›¿æ¢ 480p
    part_str = part_str.replace(" (360p)", "")  # æ›¿æ¢ 360p
    part_str = part_str.replace(" (240p)", "")  # æ›¿æ¢ 240p
    part_str = part_str.replace(" (180p)", "")  # æ›¿æ¢ 180p
    part_str = part_str.replace("  [Geo-blocked]", "")  # æ›¿æ¢[Geo-blocked]
    
    if "CCTV" in part_str and "://" not in part_str:
        part_str = part_str.replace("PLUS", "+")  # æ›¿æ¢ PLUS
        part_str = part_str.replace("1080", "")  # æ›¿æ¢ 1080
        filtered_str = ''.join(char for char in part_str if char.isdigit() or char == 'K' or char == '+')
        if not filtered_str.strip():  # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼Œå¦‚æœå‘ç°æ²¡æœ‰æ‰¾åˆ°é¢‘é“æ•°å­—è¿”å›åŸåç§°
            filtered_str = part_str.replace("CCTV", "")
        if len(filtered_str) > 2 and re.search(r'4K|8K', filtered_str):  # ç‰¹æ®Šå¤„ç†CCTVä¸­éƒ¨åˆ†4Kå’Œ8Kåç§°
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ï¼Œåˆ é™¤4Kæˆ–8Kåé¢çš„å­—ç¬¦ï¼Œå¹¶ä¸”ä¿ç•™4Kæˆ–8K
            filtered_str = re.sub(r'(4K|8K).*', r'\1', filtered_str)
            if len(filtered_str) > 2: 
                # ç»™4Kæˆ–8Kæ·»åŠ æ‹¬å·
                filtered_str = re.sub(r'(4K|8K)', r'(\1)', filtered_str)
        return "CCTV" + filtered_str 
    elif "å«è§†" in part_str:
        part_str = part_str.replace("-å«è§†", "å«è§†")  # æ›¿æ¢ -å«è§†
        # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ŒåŒ¹é…â€œå«è§†â€åé¢çš„å†…å®¹
        pattern = r'å«è§†ã€Œ.*ã€'
        # ä½¿ç”¨subå‡½æ•°æ›¿æ¢åŒ¹é…çš„å†…å®¹ä¸ºç©ºå­—ç¬¦ä¸²
        result_str = re.sub(pattern, 'å«è§†', part_str)
        return result_str
    return part_str

def filter_and_save_channel_names(input_file):
    with open(input_file, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    
    processed_lines = []
    for line in lines:
        if ',' in line:
            channel_name, url = line.split(',', 1)
            processed_channel_name = process_name_string(channel_name)
            processed_line = f"{processed_channel_name},{url}"
            processed_lines.append(processed_line)
        else:
            processed_lines.append(line)
    
    with open(input_file, 'w', encoding='utf-8') as out_file:
        for line in processed_lines:
            out_file.write(line)
            
# æ£€æµ‹URLæ˜¯å¦å¯è®¿é—®å¹¶è®°å½•å“åº”æ—¶é—´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
}
def check_url(url, timeout=2):
    try:
    	if "://" in url:
            start_time = time.time()
            response = requests.get(url, timeout=timeout, stream=True)
            # è®¡ç®—å“åº”æ—¶é—´
            elapsed_time = (time.time() - start_time) * 1000
            # å¦‚æœå“åº”çŠ¶æ€ç ä¸º200ï¼Œå³ç½‘ç«™åœ¨çº¿ï¼Œåˆ™å†™å…¥others.txt
            if response.status_code == 200:
                print(f'æ£€æµ‹æ­£å¸¸: {name},{url}, å“åº”æ—¶é—´: {elapsed_time:.2f}ms')
            return elapsed_time, True
        except requests.exceptions.Timeout:
            # å¦‚æœè¶…æ—¶ï¼Œæ‰“å°æç¤ºä¿¡æ¯
            print(f'è¶…æ—¶é”™è¯¯: {name},{url}')
        except requests.exceptions.HTTPError as e:
            # å¦‚æœHTTPè¯·æ±‚è¿”å›äº†é”™è¯¯çš„çŠ¶æ€ç 
            print(f'HTTPé”™è¯¯: {name},{url}, çŠ¶æ€ç : {e.response.status_code}')
        except requests.exceptions.TooManyRedirects:
            # å¦‚æœé‡å®šå‘æ¬¡æ•°è¿‡å¤š
            print(f'é‡å®šå‘é”™è¯¯: {name},{url}')
        except (requests.exceptions.URLRequired, requests.exceptions.MissingSchema, requests.exceptions.InvalidSchema):
            # å¦‚æœURLæ˜¯å¿…é¡»çš„ä½†æœªæä¾›ï¼Œæˆ–è€…URLçš„æ–¹æ¡ˆæ— æ•ˆ
            print(f'URLé”™è¯¯: {name},{url}')
        except requests.exceptions.ChunkedEncodingError:
            # å¦‚æœå°è¯•å¯¹ä¸æ”¯æŒåˆ†å—ç¼–ç çš„å“åº”è¿›è¡Œåˆ†å—è¯»å–
            print(f'åˆ†å—ç¼–ç é”™è¯¯: {name},{url}')
        except requests.exceptions.ContentDecodingError:
            # å¦‚æœè§£ç å“åº”å†…å®¹å¤±è´¥
            print(f'å†…å®¹è§£ç é”™è¯¯: {name},{url}')
        except requests.exceptions.RequestException as e:
            # æ‰“å°å…¶ä»–å¼‚å¸¸ä¿¡æ¯
            print(f'å…¶ä»–é”™è¯¯: {name},{url}, Error: {e}')
                
    except Exception as e:
        print(f"é¢‘é“åœ°å€æ£€æµ‹å‡ºç°é”™è¯¯ï¼š {url}: {e}")
    return None, False

# å¤„ç†å•è¡Œæ–‡æœ¬å¹¶æ£€æµ‹URL
def process_line(line):
    if "#genre#" in line or "://" not in line :
        return None, None  # è·³è¿‡åŒ…å«â€œ#genre#â€çš„è¡Œ
    parts = line.split(',')
    if len(parts) == 2:
        name, url = parts
        elapsed_time, is_valid = check_url(url.strip())
        if is_valid:
            return elapsed_time, line.strip()
        else:
            return None, line.strip()
    return None, None

# å¤šçº¿ç¨‹å¤„ç†æ–‡æœ¬å¹¶æ£€æµ‹URL
def process_urls_multithreaded(lines, max_workers=8):
    blacklist =  [] 
    successlist = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_line, line): line for line in lines}
        for future in as_completed(futures):
            elapsed_time, result = future.result()
            if result:
                if elapsed_time is not None:
                    successlist.append(f"{elapsed_time:.2f}ms,{result}")
                else:
                    blacklist.append(result)
    return successlist, blacklist

# å†™å…¥æ–‡ä»¶å†…å®¹
def write_txt_file(file_path, lines):
    with open(file_path, 'w', encoding='utf-8') as file:
        file.write('\n'.join(lines) + '\n')
            
# å†™å…¥æ–‡ä»¶
def write_list(file_path, data_list):
    with open(file_path, 'w', encoding='utf-8') as file:
        for item in data_list:
            file.write(item + '\n')

# åˆå¹¶ä¸¤ä¸ªæ–‡ä»¶çš„å†…å®¹å¹¶å†™å…¥è¾“å‡ºæ–‡ä»¶
def merge_files(file1, file2, output_file):
    lines1 = read_txt_file(file1)
    lines2 = read_txt_file(file2)
    
# åˆ é™¤é‡å¤è¡Œ
def remove_duplicates(lines, file_paths):
    for file_path in file_paths:
        file_lines = read_txt_file(file_path)
        lines = [line for line in lines if line not in file_lines]
    return lines

# å®šä¹‰äº†ä¸€ä¸ªå‡½æ•° get_comparison_setï¼Œç”¨äºä»æŒ‡å®šæ–‡ä»¶ä¸­æå– "," åçš„éƒ¨åˆ†å¹¶å­˜å…¥ä¸€ä¸ªé›†åˆã€‚
def get_comparison_set(file_path):
    comparison_set = set()
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            parts = line.split(',')
            if len(parts) > 1:
                comparison_set.add(parts[1].strip())
    return comparison_set

# å°†iptv.txtè½¬æ¢ä¸ºiptv.m3uæ–‡ä»¶
def convert_to_m3u(iptv_file, m3u_file):
    lines = read_txt(iptv_file)
    with open(m3u_file, 'w', encoding='utf-8') as file:
        file.write("#EXTM3U\n")
        for line in lines:
            parts = line.split(',', 1)
            if len(parts) == 2:
                file.write(f"#EXTINF:-1 group-title=\"{group_name}\",{parts[0]}\n")
                file.write(f"{parts[1]}\n")
                
# å¢åŠ å¤–éƒ¨urlåˆ°æ£€æµ‹æ¸…å•ï¼ŒåŒæ—¶æ”¯æŒæ£€æµ‹m3uæ ¼å¼url
# urlsé‡Œæ‰€æœ‰çš„æºéƒ½è¯»åˆ°è¿™é‡Œã€‚
urls_all_lines = []

def get_url_file_extension(url):
    # è§£æURL
    parsed_url = urlparse(url)
    # è·å–è·¯å¾„éƒ¨åˆ†
    path = parsed_url.path
    # æå–æ–‡ä»¶æ‰©å±•å
    extension = os.path.splitext(path)[1]
    return extension

def convert_m3u_to_txt(m3u_content):
    # åˆ†è¡Œå¤„ç†
    lines = m3u_content.split('\n')
    
    # ç”¨äºå­˜å‚¨ç»“æœçš„åˆ—è¡¨
    txt_lines = []
    
    # ä¸´æ—¶å˜é‡ç”¨äºå­˜å‚¨é¢‘é“åç§°
    channel_name = ""
    
    for line in lines:
        # è¿‡æ»¤æ‰ #EXTM3U å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTM3U"):
            continue
        # å¤„ç† #EXTINF å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTINF"):
            # è·å–é¢‘é“åç§°ï¼ˆå‡è®¾é¢‘é“åç§°åœ¨å¼•å·åï¼‰
            channel_name = line.split(',')[-1].strip()
        # å¤„ç† URL è¡Œ
        elif line.startswith("http"):
            txt_lines.append(f"{channel_name},{line.strip()}")
    
    # å°†ç»“æœåˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»¥æ¢è¡Œç¬¦åˆ†éš”
    return '\n'.join(txt_lines)

def process_url(url):
    try:
        # æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
        with urllib.request.urlopen(url) as response:
            # ä»¥äºŒè¿›åˆ¶æ–¹å¼è¯»å–æ•°æ®
            data = response.read()
            # å°†äºŒè¿›åˆ¶æ•°æ®è§£ç ä¸ºå­—ç¬¦ä¸²
            text = data.decode('utf-8')
            if get_url_file_extension(url)==".m3u" or get_url_file_extension(url)==".m3u8":
                urls_all_lines.append(convert_m3u_to_txt(text))
            elif get_url_file_extension(url)==".txt":
                lines = text.split('\n')
                for line in lines:
                    if  "#genre#" not in line and "," in line and "://" in line:
                        #channel_name=line.split(',')[0].strip()
                        #channel_address=line.split(',')[1].strip()
                        urls_all_lines.append(line.strip())
    
    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")


if __name__ == "__main__":
    # å®šä¹‰è¦è®¿é—®çš„å¤šä¸ªURL
    urls = [
        'https://raw.githubusercontent.com/YueChan/Live/main/IPTV.m3u',
        'https://raw.githubusercontent.com/BurningC4/Chinese-IPTV/master/TV-IPV4.m3u',
        'https://raw.githubusercontent.com/suxuang/myIPTV/main/ipv6.m3u',
        'https://raw.githubusercontent.com/YanG-1989/m3u/main/Gather.m3u',
        'https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u',
        'https://raw.githubusercontent.com/kimwang1978/collect-tv-txt/9dd572be7497b94c976e9cb62b2830770d430088/history/20240623_005422_merged_output.txt',
        'https://raw.githubusercontent.com/alonezou/yn-iptv/17b0cc6a26f2d4ded720d9350875473e4f44a65e/reference/MyIPTV',
        'https://raw.githubusercontent.com/qist/tvbox/6685fc6f7bb9b0eeece374c45a1cf406c98b70f8/tvlive.txt',
        'https://raw.githubusercontent.com/PizazzGY/TVBox_warehouse/596fdf93c9c69008b9aab0a04d42f51867726723/live.txt',
        'https://raw.githubusercontent.com/leyan1987/iptv/43fb1c8cca63bd6bebbeb0e8b77e229a2bfa2550/iptv.txt',
        'https://raw.githubusercontent.com/joevess/IPTV/main/iptv.m3u8',
        'https://raw.githubusercontent.com/Supprise0901/TVBox_live/main/live.txt',
        'https://raw.githubusercontent.com/ssili126/tv/main/itvlist.txt',
        'https://raw.githubusercontent.com/fenxp/iptv/main/live/ipv6.txt',
        'https://raw.githubusercontent.com/yuanzl77/IPTV/main/live.txt',
        'https://raw.githubusercontent.com/mlvjfchen/TV/main/iptv_list.txt',
        'https://raw.githubusercontent.com/maitel2020/iptv-self-use/main/iptv.txt',
        'https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt',
        'https://raw.githubusercontent.com/zjykfy/ykfy/main/all.m3u',
        'https://m3u.ibert.me/txt/fmml_ipv6.txt',
        'https://m3u.ibert.me/txt/fmml_dv6.txt',
        'https://m3u.ibert.me/txt/ycl_iptv.txt',
        'https://m3u.ibert.me/txt/y_g.txt',
        'https://m3u.ibert.me/txt/j_iptv.txt',
        'https://iptv-org.github.io/iptv/countries/cn.m3u',
        'https://live.fanmingming.com/tv/m3u/ipv6.m3u',
        'https://cdn.jsdelivr.net/gh/shidahuilang/shuyuan@shuyuan/iptv.txt',
        'https://gitee.com/xxy002/zhiboyuan/raw/master/zby.txt',
        'https://gitee.com/happy-is-not-closed/IPTV/raw/main/IPTV.m3u',
        'https://gitee.com/guangshanleige/iptv/raw/master/iptv.m3u',
        'https://gitlab.com/p2v5/wangtv/-/raw/main/wang-tvlive.txt',
        'https://gitlab.com/p2v5/wangtv/-/raw/main/lunbo.txt'
    ]
    for url in urls:
        print(f"å¤„ç†URL: {url}")
        process_url(url)   #è¯»å–ä¸Šé¢urlæ¸…å•ä¸­ç›´æ’­æºå­˜å…¥urls_all_lines

    # å†™å…¥ online.txt æ–‡ä»¶
    write_txt_file('online.txt',urls_all_lines)
    online_file = 'online.txt'
    filter_and_save_channel_names(online_file)

    with open('online.txt', 'r') as file:
        lines = file.readlines()

    with open('others.txt', 'w') as others_file, open('online_temp.txt', 'w') as online_file:
        for line in lines:
            if '#' in line and not any(exclude in line for exclude in ["#EXTM3U", "#EXTINF", "#genre#"]):
                name = line.split(',')[0]
                modified_line = line.replace('#', f'\n{name},')
                others_file.write(modified_line)
            else:
                online_file.write(line)
    
    # åˆå¹¶ online_temp.txt å’Œ others.txt ä¸º online.txt
    with open('online.txt', 'w') as final_online_file:
        with open('online_temp.txt', 'r') as online_temp_file:
            final_online_file.write(online_temp_file.read())
        with open('others.txt', 'r') as others_file:
            final_online_file.write(others_file.read())
            
    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    import os
    os.remove('online_temp.txt')
    os.remove('others.txt')

    print("ä¸€ä¸ªé¢‘é“å¤šä¸ªç½‘å€çš„è¡Œå·²å¤„ç†å¹¶åˆå¹¶ä¸º online.txtã€‚")

    online_file = read_txt_file('online.txt')
    
    input_file1 = 'iptv.txt'  # è¾“å…¥æ–‡ä»¶è·¯å¾„
    input_file2 = 'blacklist.txt'  # è¾“å…¥æ–‡ä»¶è·¯å¾„2 
    success_file = 'whitelist.txt'  # æˆåŠŸæ¸…å•æ–‡ä»¶è·¯å¾„
    blacklist_file = 'blacklist.txt'  # é»‘åå•æ–‡ä»¶è·¯å¾„

    # è·å– iptv.txt å’Œ blacklist.txt ä¸­çš„æ‰€æœ‰æ¯”å¯¹å†…å®¹
    iptv_set = get_comparison_set(input_file1)
    blacklist_set = get_comparison_set(input_file2)

    # åˆå¹¶å¹¶å»é‡
    merged_lines = iptv_set.union(blacklist_set)

    filtered_lines = []

    # æ¯”å¯¹ online.txt ä¸­çš„æ¯ä¸€è¡Œ
    for line in online_file:
        parts = line.split(',')
        if len(parts) > 1:
            comparison_part = parts[1].strip()
            if comparison_part not in merged_lines:
                #print(f"æ–°è·å–ç½‘å€: {comparison_part}")
                filtered_lines.append(line)

    # å°†è¿‡æ»¤åçš„å†…å®¹é‡æ–°å†™å› online.txt
    with open('online.txt', 'w', encoding='utf-8') as file:
        file.writelines(filtered_lines)

    # è¯»å–è¾“å…¥æ–‡ä»¶å†…å®¹
    lines1 = read_txt_file(input_file1)
    lines2 = read_txt_file(input_file2)
    lines=list(set(filtered_lines + lines1))
    lines = [line.strip() for line in lines if line.strip()]
    write_txt_file('tv.txt',lines)

    # æ¸…ç©º live.txt æ–‡ä»¶åè¯»å– channel.txt æ–‡ä»¶
    open('live.txt', 'w').close()
    channel_lines = read_txt('channel.txt')
    tv_lines = read_txt_file('tv.txt')

    # å¤„ç† channel.txt æ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œ
    for channel_line in channel_lines:
        if "#genre#" in channel_line:
            append_to_file('live.txt', [channel_line])
        else:
            channel_name = channel_line.split(",")[0].strip()
            print(f"Processing channel: {channel_name}")  # è°ƒè¯•ä¿¡æ¯
            matching_lines = [tv_line for tv_line in tv_lines if tv_line.split(",")[0].strip() == channel_name]
            append_to_file('live.txt', matching_lines)

    print("å¾…æ£€æµ‹æ–‡ä»¶ live.txt å·²ç”Ÿæˆã€‚")
    
    lines = read_txt_file('live.txt')

    # è®¡ç®—åˆå¹¶ååˆè®¡ä¸ªæ•°
    urls_hj = len(lines)
    
    # å¤„ç†URLå¹¶ç”ŸæˆæˆåŠŸæ¸…å•å’Œé»‘åå•
    successlist, blacklist = process_urls_multithreaded(lines)
    
    # ç»™successlist, blacklistæ’åº
    # å®šä¹‰æ’åºå‡½æ•°
    def successlist_sort_key(item):
        time_str = item.split(',')[0].replace('ms', '')
        return float(time_str)
    
    successlist=sorted(successlist, key=successlist_sort_key)
    blacklist=sorted(blacklist)

    # è®¡ç®—checkåokå’Œngä¸ªæ•°
    urls_ok = len(successlist)
    urls_ng = len(blacklist)

    # æŠŠsuccesslistæ•´ç†ä¸€ä¸‹ï¼Œç”Ÿæˆä¸€ä¸ªå¯ä»¥ç›´æ¥å¼•ç”¨çš„æºï¼Œæ–¹ä¾¿ç”¨zyplayeræ‰‹åŠ¨check
    def remove_prefix_from_lines(lines):
        result = []
        for line in lines:
            if  "#genre#" not in line and "," in line and "://" in line:
                parts = line.split(",")
                result.append(",".join(parts[1:]))
        return result

    # åŠ æ—¶é—´æˆ³ç­‰
    version=datetime.now().strftime("%Y%m%d-%H-%M-%S")+",url"
    successlist_tv = ["æ›´æ–°æ—¶é—´,#genre#"] +[version] + ['\n'] +\
                  ["whitelist,#genre#"] + remove_prefix_from_lines(successlist)
    successlist = ["æ›´æ–°æ—¶é—´,#genre#"] +[version] + ['\n'] +\
                  ["RespoTime,whitelist,#genre#"] + successlist
    blacklist = ["æ›´æ–°æ—¶é—´,#genre#"] +[version] + ['\n'] +\
                ["blacklist,#genre#"]  + blacklist

    # å†™å…¥æˆåŠŸæ¸…å•æ–‡ä»¶
    write_list(success_file, successlist_tv)
    write_list(input_file1, successlist_tv)

    # å†™å…¥é»‘åå•æ–‡ä»¶
    merged_lines = list(set(blacklist + lines2))
    merged_lines = [line.strip() for line in merged_lines if line.strip()]
    write_txt_file(blacklist_file, merged_lines)

    print(f"æˆåŠŸæ¸…å•æ–‡ä»¶å·²ç”Ÿæˆ: {success_file}")
    print(f"é»‘åå•æ–‡ä»¶å·²ç”Ÿæˆ: {blacklist_file}")
    print(f"iptv.txt æ–‡ä»¶å·²ç”Ÿæˆ: {input_file1}")

    # æ¸…ç©º iptv.txt æ–‡ä»¶åè¯»å– channel.txt æ–‡ä»¶
    channel_lines = read_txt('channel.txt')
    tv_lines = read_txt_file('iptv.txt')
    open('iptv.txt', 'w').close()

    # å¤„ç† channel.txt æ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œ
    for channel_line in channel_lines:
        if "#genre#" in channel_line:
            append_to_file('iptv.txt', [channel_line])
        else:
            channel_name = channel_line.split(",")[0].strip()
            matching_lines = [tv_line for tv_line in tv_lines if tv_line.split(",http")[0].strip() == channel_name]
            append_to_file('iptv.txt', matching_lines)
            
    # ç”Ÿæˆ iptv.m3u æ–‡ä»¶ x-tvg-url="https://raw.bgithub.xyz/Troray/IPTV/main/tvxml.xml,https://raw.bgithub.xyz/Meroser/EPG-test/main/tvxml-test.xml.gz" catchup="append" catchup-source="?playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}"

    output_text = '#EXTM3U x-tvg-url="https://raw.bgithub.xyz/Troray/IPTV/main/tvxml.xml,https://raw.bgithub.xyz/Meroser/EPG-test/main/tvxml-test.xml.gz"\n'

    with open("iptv.txt", "r", encoding='utf-8') as file:
        input_text = file.read()

    lines = input_text.strip().split("\n")
    group_name = ""
    for line in lines:
        parts = line.split(",")
        if len(parts) == 2 and "#genre#" in line:
            group_name = parts[0]
        elif len(parts) == 2:
            output_text += f"#EXTINF:-1 group-title=\"{group_name}\",{parts[0]}\n"
            output_text += f"{parts[1]}\n"

    with open("iptv.m3u", "w", encoding='utf-8') as file:
        file.write(output_text)

    print("iptv.m3uæ–‡ä»¶å·²ç”Ÿæˆã€‚")

    # æ‰§è¡Œçš„ä»£ç 
    timeend = datetime.now()

    # è®¡ç®—æ—¶é—´å·®
    elapsed_time = timeend - timestart
    total_seconds = elapsed_time.total_seconds()

    # è½¬æ¢ä¸ºåˆ†é’Ÿå’Œç§’
    minutes = int(total_seconds // 60)
    seconds = int(total_seconds % 60)

    # æ ¼å¼åŒ–å¼€å§‹å’Œç»“æŸæ—¶é—´
    timestart_str = timestart.strftime("%Y%m%d_%H_%M_%S")
    timeend_str = timeend.strftime("%Y%m%d_%H_%M_%S")

    print(f"å¼€å§‹æ—¶é—´: {timestart_str}")
    print(f"ç»“æŸæ—¶é—´: {timeend_str}")
    print(f"æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")
    print(f"urls_hj: {urls_hj} ")
    print(f"urls_ok: {urls_ok} ")
    print(f"urls_ng: {urls_ng} ")
